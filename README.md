# running-llama-locally
Brief guide to illustrate how to locally run the Llama 3 models
